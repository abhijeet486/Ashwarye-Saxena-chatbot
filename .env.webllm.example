# WebLLM Configuration Example
WEBLLM_DEFAULT_MODEL=mistralai/Mistral-7B-Instruct-v0.2
WEBLLM_TEMPERATURE=0.7
WEBLLM_MAX_TOKENS=512
WEBLLM_TOP_P=0.9
WEBLLM_GPU_MEMORY=6
WEBLLM_FALLBACK_MODE=true
WEBLLM_BATCH_SIZE=4
WEBLLM_CACHE_SIZE=100MB
# Ollama Integration
OLLAMA_BASE_URL=http://127.0.0.1:11434
OLLAMA_MODEL=llama3
OLLAMA_CHAT_ENDPOINT=http://127.0.0.1:11434/api/chat
# Main LLM Service
LLM_SERVICE_URL=http://127.0.0.1:5000/query/